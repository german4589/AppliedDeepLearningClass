{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Integrantes:\n",
    "- Jorge Eduardo Rodriguez Cardozo - 200711501\n",
    "- German Augusto Carvajal Murcia -  201313516"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 04\n",
    "\n",
    "# Fraud Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "- Fraud Detection Dataset from Microsoft Azure: [data](http://gallery.cortanaintelligence.com/Experiment/8e9fe4e03b8b4c65b9ca947c72b8e463)\n",
    "\n",
    "Fraud detection is one of the earliest industrial applications of data mining and machine learning. Fraud detection is typically handled as a binary classification problem, but the class population is unbalanced because instances of fraud are usually very rare compared to the overall volume of transactions. Moreover, when fraudulent transactions are discovered, the business typically takes measures to block the accounts from transacting to prevent further losses. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "with zipfile.ZipFile('/Users/germancarvajal/Dropbox/Universidad-201818/Deep_learning_y_redes_neuronales/AppliedDeepLearningClass/datasets/fraud_detection.csv.zip', 'r') as z:\n",
    "    f = z.open('15_fraud_detection.csv')\n",
    "    data = pd.io.parsers.read_table(f, index_col=0, sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accountAge</th>\n",
       "      <th>digitalItemCount</th>\n",
       "      <th>sumPurchaseCount1Day</th>\n",
       "      <th>sumPurchaseAmount1Day</th>\n",
       "      <th>sumPurchaseAmount30Day</th>\n",
       "      <th>paymentBillingPostalCode - LogOddsForClass_0</th>\n",
       "      <th>accountPostalCode - LogOddsForClass_0</th>\n",
       "      <th>paymentBillingState - LogOddsForClass_0</th>\n",
       "      <th>accountState - LogOddsForClass_0</th>\n",
       "      <th>paymentInstrumentAgeInAccount</th>\n",
       "      <th>ipState - LogOddsForClass_0</th>\n",
       "      <th>transactionAmount</th>\n",
       "      <th>transactionAmountUSD</th>\n",
       "      <th>ipPostalCode - LogOddsForClass_0</th>\n",
       "      <th>localHour - LogOddsForClass_0</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>720.25</td>\n",
       "      <td>5.064533</td>\n",
       "      <td>0.421214</td>\n",
       "      <td>1.312186</td>\n",
       "      <td>0.566395</td>\n",
       "      <td>3279.574306</td>\n",
       "      <td>1.218157</td>\n",
       "      <td>599.00</td>\n",
       "      <td>626.164650</td>\n",
       "      <td>1.259543</td>\n",
       "      <td>4.745402</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>62</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1185.44</td>\n",
       "      <td>2530.37</td>\n",
       "      <td>0.538996</td>\n",
       "      <td>0.481838</td>\n",
       "      <td>4.401370</td>\n",
       "      <td>4.500157</td>\n",
       "      <td>61.970139</td>\n",
       "      <td>4.035601</td>\n",
       "      <td>1185.44</td>\n",
       "      <td>1185.440000</td>\n",
       "      <td>3.981118</td>\n",
       "      <td>4.921349</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5.064533</td>\n",
       "      <td>5.096396</td>\n",
       "      <td>3.056357</td>\n",
       "      <td>3.155226</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.314186</td>\n",
       "      <td>32.09</td>\n",
       "      <td>32.090000</td>\n",
       "      <td>5.008490</td>\n",
       "      <td>4.742303</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5.064533</td>\n",
       "      <td>5.096396</td>\n",
       "      <td>3.331154</td>\n",
       "      <td>3.331239</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.529398</td>\n",
       "      <td>133.28</td>\n",
       "      <td>132.729554</td>\n",
       "      <td>1.324925</td>\n",
       "      <td>4.745402</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>132.73</td>\n",
       "      <td>5.412885</td>\n",
       "      <td>0.342945</td>\n",
       "      <td>5.563677</td>\n",
       "      <td>4.086965</td>\n",
       "      <td>0.001389</td>\n",
       "      <td>3.529398</td>\n",
       "      <td>543.66</td>\n",
       "      <td>543.660000</td>\n",
       "      <td>2.693451</td>\n",
       "      <td>4.876771</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   accountAge  digitalItemCount  sumPurchaseCount1Day  sumPurchaseAmount1Day  \\\n",
       "0        2000                 0                     0                   0.00   \n",
       "1          62                 1                     1                1185.44   \n",
       "2        2000                 0                     0                   0.00   \n",
       "3           1                 1                     0                   0.00   \n",
       "4           1                 1                     0                   0.00   \n",
       "\n",
       "   sumPurchaseAmount30Day  paymentBillingPostalCode - LogOddsForClass_0  \\\n",
       "0                  720.25                                      5.064533   \n",
       "1                 2530.37                                      0.538996   \n",
       "2                    0.00                                      5.064533   \n",
       "3                    0.00                                      5.064533   \n",
       "4                  132.73                                      5.412885   \n",
       "\n",
       "   accountPostalCode - LogOddsForClass_0  \\\n",
       "0                               0.421214   \n",
       "1                               0.481838   \n",
       "2                               5.096396   \n",
       "3                               5.096396   \n",
       "4                               0.342945   \n",
       "\n",
       "   paymentBillingState - LogOddsForClass_0  accountState - LogOddsForClass_0  \\\n",
       "0                                 1.312186                          0.566395   \n",
       "1                                 4.401370                          4.500157   \n",
       "2                                 3.056357                          3.155226   \n",
       "3                                 3.331154                          3.331239   \n",
       "4                                 5.563677                          4.086965   \n",
       "\n",
       "   paymentInstrumentAgeInAccount  ipState - LogOddsForClass_0  \\\n",
       "0                    3279.574306                     1.218157   \n",
       "1                      61.970139                     4.035601   \n",
       "2                       0.000000                     3.314186   \n",
       "3                       0.000000                     3.529398   \n",
       "4                       0.001389                     3.529398   \n",
       "\n",
       "   transactionAmount  transactionAmountUSD  ipPostalCode - LogOddsForClass_0  \\\n",
       "0             599.00            626.164650                          1.259543   \n",
       "1            1185.44           1185.440000                          3.981118   \n",
       "2              32.09             32.090000                          5.008490   \n",
       "3             133.28            132.729554                          1.324925   \n",
       "4             543.66            543.660000                          2.693451   \n",
       "\n",
       "   localHour - LogOddsForClass_0  Label  \n",
       "0                       4.745402      0  \n",
       "1                       4.921349      0  \n",
       "2                       4.742303      0  \n",
       "3                       4.745402      0  \n",
       "4                       4.876771      0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((138721, 16), 797, 0.0057453449730033666)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape, data.Label.sum(), data.Label.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop(['Label'], axis=1)\n",
    "y = data['Label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercice 04.1\n",
    "\n",
    "Estimate a Logistic Regression\n",
    "\n",
    "Evaluate using the following metrics:\n",
    "* Accuracy\n",
    "* F1-Score\n",
    "* F_Beta-Score (Beta=10)\n",
    "\n",
    "Comment about the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acuracy: 0.993973645512\n",
      "precision_score  0.0\n",
      "recall_score     0.0\n",
      "f1_score     0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/germancarvajal/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/germancarvajal/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "random.seed(123)\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train,y_train)\n",
    "y_pred_class=logreg.predict(X_test)\n",
    "from sklearn import metrics\n",
    "print('Acuracy: '+ str(metrics.accuracy_score(y_test, y_pred_class)))\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "print('precision_score ', precision_score(y_test, y_pred_class))\n",
    "print('recall_score    ', recall_score(y_test, y_pred_class))\n",
    "print('f1_score    ', f1_score(y_test, y_pred_class))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Bad performance due to an unbalanced sample. There's no detection af the postitive class ant thus is not posible to cumpute the precesion or recall score, as well as the undetermined f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercice 04.2\n",
    "\n",
    "Under-sample the negative class using random-under-sampling\n",
    "\n",
    "Which parameter for target_percentage did you choose?\n",
    "How the results change?\n",
    "\n",
    "**Only apply under-sampling to the training set, evaluate using the whole test set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def UnderSampling(X, y, target_percentage=0.5, seed=None):\n",
    "    # Assuming minority class is the positive\n",
    "    n_samples = y.shape[0]\n",
    "    n_samples_0 = (y == 0).sum()\n",
    "    n_samples_1 = (y == 1).sum()\n",
    "\n",
    "    n_samples_0_new =  n_samples_1 / target_percentage - n_samples_1\n",
    "    n_samples_0_new_per = n_samples_0_new / n_samples_0\n",
    "\n",
    "    filter_ = y == 0\n",
    "\n",
    "    np.random.seed(seed)\n",
    "    rand_1 = np.random.binomial(n=1, p=n_samples_0_new_per, size=n_samples)\n",
    "    \n",
    "    filter_ = filter_ & rand_1\n",
    "    filter_ = filter_ | (y == 1)\n",
    "    filter_ = filter_.astype(bool)\n",
    "    \n",
    "    return X[filter_], y[filter_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acuracy: 0.491796660996\n",
      "precision_score  0.00957422842983\n",
      "recall_score     0.813397129187\n",
      "f1_score     0.0189256888394\n"
     ]
    }
   ],
   "source": [
    "target_percentage = 0.5\n",
    "X_u, y_u = UnderSampling(X_train, y_train, target_percentage, 123)\n",
    "logreg_under = LogisticRegression()\n",
    "logreg_under.fit(X_u,y_u)\n",
    "y_pred_class=logreg_under.predict(X_test)\n",
    "print('Acuracy: '+ str(metrics.accuracy_score(y_test, y_pred_class)))\n",
    "print('precision_score ', precision_score(y_test, y_pred_class))\n",
    "print('recall_score    ', recall_score(y_test, y_pred_class))\n",
    "print('f1_score    ', f1_score(y_test, y_pred_class))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The prediction improves due to the balancing of the sample. Now the algorithm can detect and predict the postive class, but with a high reate of false positive detections resulting in a low precision score compared to the over 80% recall score.\n",
    "\n",
    "> The F1 score is computed with a value of just 0.018 then making it too low for the model to be used as a prediction mechanism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercice 04.3\n",
    "\n",
    "Now using random-over-sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def OverSampling(X, y, target_percentage=0.5, seed=None):\n",
    "    # Assuming minority class is the positive\n",
    "    n_samples = y.shape[0]\n",
    "    n_samples_0 = (y == 0).sum()\n",
    "    n_samples_1 = (y == 1).sum()\n",
    "    \n",
    "    n_samples_1_new =  target_percentage * n_samples_0 / (1-target_percentage)\n",
    "\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    filter_ = np.random.choice(X[y == 1].index, int(n_samples_1_new))\n",
    "\n",
    "    filter_ = np.hstack((filter_,X[y == 0].index))\n",
    "\n",
    "    return X.loc[filter_], y.loc[filter_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acuracy: 0.530953548052\n",
      "precision_score  0.00970933072789\n",
      "recall_score     0.760765550239\n",
      "f1_score     0.0191739523666\n"
     ]
    }
   ],
   "source": [
    "target_percentage = 0.5\n",
    "X_o, y_o = OverSampling(X_train, y_train, target_percentage, 123)\n",
    "logreg_over = LogisticRegression()\n",
    "logreg_over.fit(X_o,y_o)\n",
    "y_pred_class=logreg_over.predict(X_test)\n",
    "print('Acuracy: '+ str(metrics.accuracy_score(y_test, y_pred_class)))\n",
    "print('precision_score ', precision_score(y_test, y_pred_class))\n",
    "print('recall_score    ', recall_score(y_test, y_pred_class))\n",
    "print('f1_score    ', f1_score(y_test, y_pred_class))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The results of the over sampling present a small improvement in the precision potentialy due to the fact of having more positve class individuals to learn the patter. Yet the recall score is lower, meaning that the improvement in the precision is completely attributable to a reduction of prediction of positive classes registries.\n",
    "\n",
    "> The F1 score is slightly superior but still too low to use the model as a reliable classification mechanism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercice 04.4*\n",
    "Evaluate the results using SMOTE\n",
    "\n",
    "Which parameters did you choose?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SMOTE(X, y, target_percentage=0.5, k=5, seed=None):\n",
    "    \n",
    "    n_samples = y.shape[0]\n",
    "    n_samples_0 = (y == 0).sum()\n",
    "    n_samples_1 = (y == 1).sum()\n",
    "    \n",
    "    # New samples\n",
    "    n_samples_1_new =  int(-target_percentage * n_samples_0 / (target_percentage- 1) - n_samples_1)\n",
    "    \n",
    "    # A matrix to store the synthetic samples\n",
    "    new = np.zeros((n_samples_1_new, X.shape[1]))\n",
    "\n",
    "    # Create seeds\n",
    "    np.random.seed(seed)\n",
    "    seeds = np.random.randint(1, 1000000, 3)\n",
    "   \n",
    "    # Select examples to use as base\n",
    "    np.random.seed(seeds[0])\n",
    "    sel_ = np.random.choice(y[y==1].shape[0], n_samples_1_new)\n",
    "    \n",
    "    # Define random seeds (2 per example)\n",
    "    np.random.seed(seeds[1])\n",
    "    nn__ = np.random.choice(k, n_samples_1_new)\n",
    "    np.random.seed(seeds[2])\n",
    "    steps = np.random.uniform(size=n_samples_1_new)  \n",
    "\n",
    "    # For each selected examples create one synthetic case\n",
    "    for i, sel in enumerate(sel_):\n",
    "        # Select neighbor\n",
    "        nn_ = nn__[i]\n",
    "        step = steps[i]\n",
    "        # Create new sample\n",
    "        new[i, :] = X[y==1].iloc[sel] - step * (X[y==1].iloc[sel] - X[y==1].iloc[nn_])\n",
    "    \n",
    "    X = np.vstack((X, new))\n",
    "    y = np.append(y, np.ones(n_samples_1_new))\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acuracy: 0.70981228915\n",
      "precision_score  0.0073977806658\n",
      "recall_score     0.354066985646\n",
      "f1_score     0.0144927536232\n"
     ]
    }
   ],
   "source": [
    "target_percentage = 0.5\n",
    "X_s,y_s=SMOTE(X_train, y_train, target_percentage, 5, 123)\n",
    "logreg_smote = LogisticRegression()\n",
    "logreg_smote.fit(X_s,y_s)\n",
    "y_pred_class=logreg_smote.predict(X_test)\n",
    "print('Acuracy: '+ str(metrics.accuracy_score(y_test, y_pred_class)))\n",
    "print('precision_score ', precision_score(y_test, y_pred_class))\n",
    "print('recall_score    ', recall_score(y_test, y_pred_class))\n",
    "print('f1_score    ', f1_score(y_test, y_pred_class))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Even thought SMOTE should have included a far grater amount of information into the database variability, the performance of the trained algorithm is worse that that of the under sampled and over sampled trials. In the particular, the model has a clear biased toward predicting false positive individuals then reducing significally the precision score. At the same time, the recall score takes a hit on the performance side due to the low detection capacity of true positives within the sample.\n",
    "\n",
    "> The F1 score is lower than the models trained with over and under sampling. Adding to the fact the method requires havey computational time, the performance result render useless the use of SMOTE to fight the unbalancing of the classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercice 04.5\n",
    "\n",
    "Estimate a Logistic Regression, GaussianNB, K-nearest neighbors and a Decision Tree **Classifiers**\n",
    "\n",
    "Evaluate using the following metrics:\n",
    "* Accuracy\n",
    "* F1-Score\n",
    "* F_Beta-Score (Beta=10)\n",
    "\n",
    "Comment about the results\n",
    "\n",
    "Combine the classifiers and comment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** - The models are trained with the over-sampled database because is was the input for the best performance logistic regression model evaluated **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "models = {'lr': LogisticRegression(),\n",
    "          'dt': DecisionTreeClassifier(),\n",
    "          'nb': GaussianNB(),\n",
    "          'nn': KNeighborsClassifier()}\n",
    "\n",
    "for model in models.keys():\n",
    "    models[model].fit(X_o, y_o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = pd.DataFrame(index=X_test.index, columns=models.keys())\n",
    "for model in models.keys():\n",
    "    y_pred[model] = models[model].predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr\n",
      "Acuracy: 0.530953548052\n",
      "f1_score     0.0191739523666\n",
      "F_beta_score     0.430813391995\n",
      "dt\n",
      "Acuracy: 0.989706179176\n",
      "f1_score     0.118518518519\n",
      "F_beta_score     0.114903299204\n",
      "nb\n",
      "Acuracy: 0.222311928722\n",
      "f1_score     0.0140376530799\n",
      "F_beta_score     0.403613204013\n",
      "nn\n",
      "Acuracy: 0.98082523572\n",
      "f1_score     0.104979811575\n",
      "F_beta_score     0.183773444061\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import fbeta_score\n",
    "for model in models.keys():\n",
    "    print(model)\n",
    "    print('Acuracy: '+ str(metrics.accuracy_score(y_test, y_pred[model])))\n",
    "    print('f1_score    ', f1_score(y_test, y_pred[model]))\n",
    "    print('F_beta_score    ', fbeta_score(y_test, y_pred[model],beta=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** - The assembly method is the hard voting majority **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('lr', LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)), ('dt', Decisio...owski',\n",
       "           metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
       "           weights='uniform'))],\n",
       "         flatten_transform=None, n_jobs=1, voting='hard', weights=None)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "ensemble = VotingClassifier(estimators=[('lr', models['lr']), ('dt', models['dt']), ('nb', models['nb']),('nn',models['nn'])], voting='hard')\n",
    "ensemble.fit(X_o,y_o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_vot=ensemble.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acuracy: 0.982641792336\n",
      "f1_score     0.117302052786\n",
      "F_beta_score     0.189023534366\n"
     ]
    }
   ],
   "source": [
    "print('Acuracy: '+ str(metrics.accuracy_score(y_test, y_pred_vot)))\n",
    "print('f1_score    ', f1_score(y_test, y_pred_vot))\n",
    "print('F_beta_score    ', fbeta_score(y_test, y_pred_vot,beta=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Each of the classifiers trained exhibits a low performance in terms of the F1 score, even in those which have a high accuracy due to the biased towards predicting false positives within the sample. The ensamble model suffers the same problem with an acuracy of over 98% but a F1 score of just 0.11.\n",
    "\n",
    "> Thought the combined model has a higher F1 and F_beta score over the two worst performers, the aggregate classfication performance is still worse than the best model (The decission tree classifier)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercice 04.6\n",
    "\n",
    "Using the under-sampled dataset\n",
    "\n",
    "Evaluate a RandomForestClassifier and compare the results\n",
    "\n",
    "change n_estimators=100, what happened"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "AUC = []\n",
    "estimator_range=range(10,160,10)\n",
    "rfreg = RandomForestClassifier()\n",
    "for estimator in estimator_range:\n",
    "    rfclass = RandomForestClassifier(n_estimators=estimator, random_state=123, n_jobs=-1)\n",
    "    rfclass.fit(X_u,y_u)\n",
    "    AUC.append(metrics.roc_auc_score(y_test, rfclass.predict_proba(X_test)[:,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training multiple random forest classfiers with different number of individual trees results in the following graph where the y-axis corresponds to the computed area-under-ROC curve. Using this rough approximation is possible to observe the effects of an invresing number of individual classifiers over the total prediction power of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0,0.5,'AUC (Higher is better)')"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(estimator_range, AUC)\n",
    "plt.xlabel('n_estimators')\n",
    "plt.ylabel('AUC (Higher is better)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking a risk it is even possible to assure that the \"optimum\" number of trees is o is close to 130 classification trees grown within the random forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "130"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimator_range[AUC.index(max(AUC))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the random forest with 10 individual trees to begin, leads to a poor prediction power model with a strong bias towards false positive detection. Yet the total recall is acceptable, identifying correctly 63% of the real positives in the sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 trees\n",
      "Acuracy: 0.778783772094\n",
      "precision_score  0.01720791823\n",
      "recall_score     0.636363636364\n",
      "f1_score     0.0335097001764\n",
      "F_beta_score     0.469209542771\n"
     ]
    }
   ],
   "source": [
    "print('10 trees')\n",
    "rfclass = RandomForestClassifier(n_estimators=10, random_state=123, n_jobs=-1)\n",
    "rfclass.fit(X_u,y_u)\n",
    "print('Acuracy: '+ str(metrics.accuracy_score(y_test, rfclass.predict(X_test))))\n",
    "print('precision_score ', precision_score(y_test, rfclass.predict(X_test)))\n",
    "print('recall_score    ', recall_score(y_test, rfclass.predict(X_test)))\n",
    "print('f1_score    ', f1_score(y_test, rfclass.predict(X_test)))\n",
    "print('F_beta_score    ', fbeta_score(y_test, rfclass.predict(X_test),beta=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By increasing the number of trees to a 100 there is a small increase in the recall now with an approximate value of 69%, even when all the other computed performance metric remain unchanged by the increase in the model size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 trees\n",
      "Acuracy: 0.76301144719\n",
      "precision_score  0.0173535791757\n",
      "recall_score     0.688995215311\n",
      "f1_score     0.0338544727871\n",
      "F_beta_score     0.498116309336\n"
     ]
    }
   ],
   "source": [
    "print('100 trees')\n",
    "rfclass = RandomForestClassifier(n_estimators=100, random_state=123, n_jobs=-1)\n",
    "rfclass.fit(X_u,y_u)\n",
    "print('Acuracy: '+ str(metrics.accuracy_score(y_test, rfclass.predict(X_test))))\n",
    "print('precision_score ', precision_score(y_test, rfclass.predict(X_test)))\n",
    "print('recall_score    ', recall_score(y_test, rfclass.predict(X_test)))\n",
    "print('f1_score    ', f1_score(y_test, rfclass.predict(X_test)))\n",
    "print('F_beta_score    ', fbeta_score(y_test, rfclass.predict(X_test),beta=10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
